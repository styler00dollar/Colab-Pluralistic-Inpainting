{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-Pluralistic-Inpainting ",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQZtZrWQPm-P",
        "colab_type": "text"
      },
      "source": [
        "# Colab-Plurastic-Inpainting\n",
        "Original repo: [lyndonzheng/Pluralistic-Inpainting](https://github.com/lyndonzheng/Pluralistic-Inpainting)\n",
        "\n",
        "Differentiable Augmentation: [mit-han-lab/data-efficient-gans](https://github.com/mit-han-lab/data-efficient-gans)\n",
        "\n",
        "My fork: [styler00dollar /Colab-Pluralistic-Inpainting](https://github.com/styler00dollar/Colab-Pluralistic-Inpainting)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs_rXTu0pOkl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGQquWaDNUJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/lyndonzheng/Pluralistic-Inpainting\n",
        "!pip install visdom dominate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rhzbKhbpDaF",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title base options\n",
        "%%writefile /content/Pluralistic-Inpainting/options/base_options.py\n",
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "import model\n",
        "from util import util\n",
        "\n",
        "\n",
        "class BaseOptions():\n",
        "    def __init__(self):\n",
        "        self.parser = argparse.ArgumentParser()\n",
        "        self.initialized = False\n",
        "\n",
        "    def initialize(self, parser):\n",
        "        # base define\n",
        "        parser.add_argument('--name', type=str, default='experiment_name', help='name of the experiment.')\n",
        "        parser.add_argument('--model', type=str, default='pluralistic', help='name of the model type. [pluralistic]')\n",
        "        parser.add_argument('--mask_type', type=int, default=[1,2],\n",
        "                            help='mask type, 0: center mask, 1:random regular mask, '\n",
        "                            '2: random irregular mask. 3: external irregular mask. [0],[1,2],[1,2,3]')\n",
        "        parser.add_argument('--checkpoints_dir', type=str, default='./checkpoints', help='models are save here')\n",
        "        parser.add_argument('--which_iter', type=str, default='latest', help='which iterations to load')\n",
        "        parser.add_argument('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0, 1, 2 use -1 for CPU')\n",
        "\n",
        "        # data pattern define\n",
        "        parser.add_argument('--img_file', type=str, default='/data/dataset/train', help='training and testing dataset')\n",
        "        parser.add_argument('--mask_file', type=str, default='none', help='load test mask')\n",
        "        parser.add_argument('--loadSize', type=int, default=[266, 266], help='scale images to this size') #default=[266, 266]\n",
        "        parser.add_argument('--fineSize', type=int, default=[256, 256], help='then crop to this size') #default=[256, 256]\n",
        "        parser.add_argument('--resize_or_crop', type=str, default='resize_and_crop', help='scaling and cropping of images at load time [resize_and_crop|crop|]')\n",
        "        parser.add_argument('--no_flip', action='store_true', help='if specified, do not flip the image for data augmentation')\n",
        "        parser.add_argument('--no_rotation', action='store_true', help='if specified, do not rotation for data augmentation')\n",
        "        parser.add_argument('--no_augment', action='store_true', help='if specified, do not augment the image for data augmentation')\n",
        "        parser.add_argument('--batchSize', type=int, default=8, help='input batch size')\n",
        "        parser.add_argument('--nThreads', type=int, default=8, help='# threads for loading data')\n",
        "        parser.add_argument('--no_shuffle', action='store_true',help='if true, takes images serial')\n",
        "\n",
        "        # display parameter define\n",
        "        parser.add_argument('--display_winsize', type=int, default=256, help='display window size')\n",
        "        parser.add_argument('--display_id', type=int, default=1, help='display id of the web')\n",
        "        parser.add_argument('--display_port', type=int, default=8097, help='visidom port of the web display')\n",
        "        parser.add_argument('--display_single_pane_ncols', type=int, default=0, help='if positive, display all images in a single visidom web panel')\n",
        "\n",
        "        return parser\n",
        "\n",
        "    def gather_options(self):\n",
        "        \"\"\"Add additional model-specific options\"\"\"\n",
        "\n",
        "        if not self.initialized:\n",
        "            parser = self.initialize(self.parser)\n",
        "\n",
        "        # get basic options\n",
        "        opt, _ = parser.parse_known_args()\n",
        "\n",
        "        # modify the options for different models\n",
        "        model_option_set = model.get_option_setter(opt.model)\n",
        "        parser = model_option_set(parser, self.isTrain)\n",
        "        opt = parser.parse_args()\n",
        "\n",
        "        return opt\n",
        "\n",
        "    def parse(self):\n",
        "        \"\"\"Parse the options\"\"\"\n",
        "\n",
        "        opt = self.gather_options()\n",
        "        opt.isTrain = self.isTrain\n",
        "\n",
        "        self.print_options(opt)\n",
        "\n",
        "        # set gpu ids\n",
        "        str_ids = opt.gpu_ids.split(',')\n",
        "        opt.gpu_ids = []\n",
        "        for str_id in str_ids:\n",
        "            id = int(str_id)\n",
        "            if id >= 0:\n",
        "                opt.gpu_ids.append(id)\n",
        "        if len(opt.gpu_ids):\n",
        "            torch.cuda.set_device(opt.gpu_ids[0])\n",
        "\n",
        "        self.opt = opt\n",
        "\n",
        "        return self.opt\n",
        "\n",
        "    @staticmethod\n",
        "    def print_options(opt):\n",
        "        \"\"\"print and save options\"\"\"\n",
        "\n",
        "        print('--------------Options--------------')\n",
        "        for k, v in sorted(vars(opt).items()):\n",
        "            print('%s: %s' % (str(k), str(v)))\n",
        "        print('----------------End----------------')\n",
        "\n",
        "        # save to the disk\n",
        "        expr_dir = os.path.join(opt.checkpoints_dir, opt.name)\n",
        "        util.mkdirs(expr_dir)\n",
        "        if opt.isTrain:\n",
        "            file_name = os.path.join(expr_dir, 'train_opt.txt')\n",
        "        else:\n",
        "            file_name = os.path.join(expr_dir, 'test_opt.txt')\n",
        "        with open(file_name, 'wt') as opt_file:\n",
        "            opt_file.write('--------------Options--------------\\n')\n",
        "            for k, v in sorted(vars(opt).items()):\n",
        "                opt_file.write('%s: %s\\n' % (str(k), str(v)))\n",
        "            opt_file.write('----------------End----------------\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMeXZvYEqi25",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Download models\n",
        "!pip install gdown\n",
        "# places random\n",
        "!mkdir /content/Pluralistic-Inpainting/checkpoints/place2_random\n",
        "%cd /content/Pluralistic-Inpainting/checkpoints/place2_random\n",
        "!gdown --id 1gfpcxyypQ2vbA25oK3bvGtT_-36uNtGT\n",
        "!gdown --id 1B__248OMdH7VLnA56kmjzMcoP9Erasq8\n",
        "!gdown --id 157lwR5RpZAI6E2H4qo2dlB5dAXl1exfF\n",
        "!gdown --id 1yT-l9HWGKFJqmusAFo-429OMLWGz5tmq\n",
        "# celeba random\n",
        "!mkdir /content/Pluralistic-Inpainting/checkpoints/celeba_random\n",
        "%cd /content/Pluralistic-Inpainting/checkpoints/celeba_random\n",
        "!gdown --id 1utryzvzh1HNHZWwfXWp0ifO14fYrsTEl\n",
        "!gdown --id 1MC4o3kM5Cc9QcW69ly_4IQVH3bnjMDY-\n",
        "!gdown --id 1RWhd4jMeJK3dHE0Upf8TN1dPAiuw_r9M\n",
        "!gdown --id 1RkEqDwaWvr57gFDic9z_PVA4LvFhu_II\n",
        "# imagenet random\n",
        "!mkdir /content/Pluralistic-Inpainting/checkpoints/imagenet_random\n",
        "%cd /content/Pluralistic-Inpainting/checkpoints/imagenet_random\n",
        "!gdown --id 1V1pbf6Y4sukZITswQJBKo4Z9J24zc2Mn\n",
        "!gdown --id 1hh80HCEQmq5mnXKfVOgwuCwdweaev0r0\n",
        "!gdown --id 17VX9sDFiuwp0dwRRITgHR1Hd8nDVjQqa\n",
        "!gdown --id 17X5sBxk3B2SB0jNzlKBCpFTNEynzHPYr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHSC0XIxpMeP",
        "colab_type": "text"
      },
      "source": [
        "Set --mask_type in options/base_options.py to test various masks. --mask_file path is needed for 3. external irregular mask,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUvMZTWbpH5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/Pluralistic-Inpainting\n",
        "!python test.py  --name place2_random --img_file /content/input/ --mask_file /content/mask #--no_flip --no_rotation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8mdDaeFQES7",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb7tueiQLKjS",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title differentiable augmentation (experimental)\n",
        "%%writefile /content/Pluralistic-Inpainting/model/pluralistic_model.py\n",
        "# Differentiable Augmentation for Data-Efficient GAN Training\n",
        "# Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n",
        "# https://arxiv.org/pdf/2006.10738\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def DiffAugment(x, policy='', channels_first=True):\n",
        "    if policy:\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 3, 1, 2)\n",
        "        for p in policy.split(','):\n",
        "            for f in AUGMENT_FNS[p]:\n",
        "                x = f(x)\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 2, 3, 1)\n",
        "        x = x.contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_brightness(x):\n",
        "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_saturation(x):\n",
        "    x_mean = x.mean(dim=1, keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_contrast(x):\n",
        "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_translation(x, ratio=0.125):\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_cutout(x, ratio=0.5):\n",
        "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "    mask[grid_batch, grid_x, grid_y] = 0\n",
        "    x = x * mask.unsqueeze(1)\n",
        "    return x\n",
        "\n",
        "\n",
        "AUGMENT_FNS = {\n",
        "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
        "    'translation': [rand_translation],\n",
        "    'cutout': [rand_cutout],\n",
        "}\n",
        "\n",
        "policy = 'color,translation,cutout'\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "from .base_model import BaseModel\n",
        "from . import network, base_function, external_function\n",
        "from util import task\n",
        "import itertools\n",
        "\n",
        "\n",
        "class Pluralistic(BaseModel):\n",
        "    \"\"\"This class implements the pluralistic image completion, for 256*256 resolution image inpainting\"\"\"\n",
        "    def name(self):\n",
        "        return \"Pluralistic Image Completion\"\n",
        "\n",
        "    @staticmethod\n",
        "    def modify_options(parser, is_train=True):\n",
        "        \"\"\"Add new options and rewrite default values for existing options\"\"\"\n",
        "        parser.add_argument('--output_scale', type=int, default=4, help='# of number of the output scale')\n",
        "        if is_train:\n",
        "            parser.add_argument('--train_paths', type=str, default='two', help='training strategies with one path or two paths')\n",
        "            parser.add_argument('--lambda_rec', type=float, default=20.0, help='weight for image reconstruction loss')\n",
        "            parser.add_argument('--lambda_kl', type=float, default=20.0, help='weight for kl divergence loss')\n",
        "            parser.add_argument('--lambda_g', type=float, default=1.0, help='weight for generation loss')\n",
        "\n",
        "        return parser\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        \"\"\"Initial the pluralistic model\"\"\"\n",
        "        BaseModel.__init__(self, opt)\n",
        "\n",
        "        self.loss_names = ['kl_rec', 'kl_g', 'app_rec', 'app_g', 'ad_g', 'img_d', 'ad_rec', 'img_d_rec']\n",
        "        self.visual_names = ['img_m', 'img_c', 'img_truth', 'img_out', 'img_g', 'img_rec']\n",
        "        self.value_names = ['u_m', 'sigma_m', 'u_post', 'sigma_post', 'u_prior', 'sigma_prior']\n",
        "        self.model_names = ['E', 'G', 'D', 'D_rec']\n",
        "        self.distribution = []\n",
        "\n",
        "        # define the inpainting model\n",
        "        self.net_E = network.define_e(ngf=32, z_nc=128, img_f=128, layers=5, norm='none', activation='LeakyReLU',\n",
        "                                      init_type='orthogonal', gpu_ids=opt.gpu_ids)\n",
        "        self.net_G = network.define_g(ngf=32, z_nc=128, img_f=128, L=0, layers=5, output_scale=opt.output_scale,\n",
        "                                      norm='instance', activation='LeakyReLU', init_type='orthogonal', gpu_ids=opt.gpu_ids)\n",
        "        # define the discriminator model\n",
        "        self.net_D = network.define_d(ndf=32, img_f=128, layers=5, model_type='ResDis', init_type='orthogonal', gpu_ids=opt.gpu_ids)\n",
        "        self.net_D_rec = network.define_d(ndf=32, img_f=128, layers=5, model_type='ResDis', init_type='orthogonal', gpu_ids=opt.gpu_ids)\n",
        "\n",
        "        if self.isTrain:\n",
        "            # define the loss functions\n",
        "            self.GANloss = external_function.GANLoss(opt.gan_mode)\n",
        "            self.L1loss = torch.nn.L1Loss()\n",
        "            self.L2loss = torch.nn.MSELoss()\n",
        "            # define the optimizer\n",
        "            self.optimizer_G = torch.optim.Adam(itertools.chain(filter(lambda p: p.requires_grad, self.net_G.parameters()),\n",
        "                        filter(lambda p: p.requires_grad, self.net_E.parameters())), lr=opt.lr, betas=(0.0, 0.999))\n",
        "            self.optimizer_D = torch.optim.Adam(itertools.chain(filter(lambda p: p.requires_grad, self.net_D.parameters()),\n",
        "                                                filter(lambda p: p.requires_grad, self.net_D_rec.parameters())),\n",
        "                                                lr=opt.lr, betas=(0.0, 0.999))\n",
        "            self.optimizers.append(self.optimizer_G)\n",
        "            self.optimizers.append(self.optimizer_D)\n",
        "        # load the pretrained model and schedulers\n",
        "        self.setup(opt)\n",
        "\n",
        "    def set_input(self, input, epoch=0):\n",
        "        \"\"\"Unpack input data from the data loader and perform necessary pre-process steps\"\"\"\n",
        "        self.input = input\n",
        "        self.image_paths = self.input['img_path']\n",
        "        self.img = input['img']\n",
        "        self.mask = input['mask']\n",
        "\n",
        "        if len(self.gpu_ids) > 0:\n",
        "            self.img = self.img.cuda(self.gpu_ids[0], async=True)\n",
        "            self.mask = self.mask.cuda(self.gpu_ids[0], async=True)\n",
        "\n",
        "        # get I_m and I_c for image with mask and complement regions for training\n",
        "        self.img_truth = self.img * 2 - 1\n",
        "        self.img_m = self.mask * self.img_truth\n",
        "        self.img_c = (1 - self.mask) * self.img_truth\n",
        "\n",
        "        # get multiple scales image ground truth and mask for training\n",
        "        self.scale_img = task.scale_pyramid(self.img_truth, self.opt.output_scale)\n",
        "        self.scale_mask = task.scale_pyramid(self.mask, self.opt.output_scale)\n",
        "\n",
        "    def test(self):\n",
        "        \"\"\"Forward function used in test time\"\"\"\n",
        "        # save the groundtruth and masked image\n",
        "        self.save_results(self.img_truth, data_name='truth')\n",
        "        self.save_results(self.img_m, data_name='mask')\n",
        "\n",
        "        # encoder process\n",
        "        distribution, f = self.net_E(self.img_m)\n",
        "        q_distribution = torch.distributions.Normal(distribution[-1][0], distribution[-1][1])\n",
        "        scale_mask = task.scale_img(self.mask, size=[f[2].size(2), f[2].size(3)])\n",
        "\n",
        "        # decoder process\n",
        "        for i in range(self.opt.nsampling):\n",
        "            z = q_distribution.sample()\n",
        "            self.img_g, attn = self.net_G(z, f_m=f[-1], f_e=f[2], mask=scale_mask.chunk(3, dim=1)[0])\n",
        "            self.img_out = (1 - self.mask) * self.img_g[-1].detach() + self.mask * self.img_m\n",
        "            self.score = self.net_D(self.img_out)\n",
        "            self.save_results(self.img_out, i, data_name='out')\n",
        "\n",
        "    def get_distribution(self, distributions):\n",
        "        \"\"\"Calculate encoder distribution for img_m, img_c\"\"\"\n",
        "        # get distribution\n",
        "        sum_valid = (torch.mean(self.mask.view(self.mask.size(0), -1), dim=1) - 1e-5).view(-1, 1, 1, 1)\n",
        "        m_sigma = 1 / (1 + ((sum_valid - 0.8) * 8).exp_())\n",
        "        p_distribution, q_distribution, kl_rec, kl_g = 0, 0, 0, 0\n",
        "        self.distribution = []\n",
        "        for distribution in distributions:\n",
        "            p_mu, p_sigma, q_mu, q_sigma = distribution\n",
        "            # the assumption distribution for different mask regions\n",
        "            m_distribution = torch.distributions.Normal(torch.zeros_like(p_mu), m_sigma * torch.ones_like(p_sigma))\n",
        "            # m_distribution = torch.distributions.Normal(torch.zeros_like(p_mu), torch.ones_like(p_sigma))\n",
        "            # the post distribution from mask regions\n",
        "            p_distribution = torch.distributions.Normal(p_mu, p_sigma)\n",
        "            p_distribution_fix = torch.distributions.Normal(p_mu.detach(), p_sigma.detach())\n",
        "            # the prior distribution from valid region\n",
        "            q_distribution = torch.distributions.Normal(q_mu, q_sigma)\n",
        "\n",
        "            # kl divergence\n",
        "            kl_rec += torch.distributions.kl_divergence(m_distribution, p_distribution)\n",
        "            if self.opt.train_paths == \"one\":\n",
        "                kl_g += torch.distributions.kl_divergence(m_distribution, q_distribution)\n",
        "            elif self.opt.train_paths == \"two\":\n",
        "                kl_g += torch.distributions.kl_divergence(p_distribution_fix, q_distribution)\n",
        "            self.distribution.append([torch.zeros_like(p_mu), m_sigma * torch.ones_like(p_sigma), p_mu, p_sigma, q_mu, q_sigma])\n",
        "\n",
        "        return p_distribution, q_distribution, kl_rec, kl_g\n",
        "\n",
        "    def get_G_inputs(self, p_distribution, q_distribution, f):\n",
        "        \"\"\"Process the encoder feature and distributions for generation network\"\"\"\n",
        "        f_m = torch.cat([f[-1].chunk(2)[0], f[-1].chunk(2)[0]], dim=0)\n",
        "        f_e = torch.cat([f[2].chunk(2)[0], f[2].chunk(2)[0]], dim=0)\n",
        "        scale_mask = task.scale_img(self.mask, size=[f_e.size(2), f_e.size(3)])\n",
        "        mask = torch.cat([scale_mask.chunk(3, dim=1)[0], scale_mask.chunk(3, dim=1)[0]], dim=0)\n",
        "        z_p = p_distribution.rsample()\n",
        "        z_q = q_distribution.rsample()\n",
        "        z = torch.cat([z_p, z_q], dim=0)\n",
        "        return z, f_m, f_e, mask\n",
        "\n",
        "    def forward(self):\n",
        "        \"\"\"Run forward processing to get the inputs\"\"\"\n",
        "        # encoder process\n",
        "        distributions, f = self.net_E(self.img_m, self.img_c)\n",
        "        p_distribution, q_distribution, self.kl_rec, self.kl_g = self.get_distribution(distributions)\n",
        "\n",
        "        # decoder process\n",
        "        z, f_m, f_e, mask = self.get_G_inputs(p_distribution, q_distribution, f)\n",
        "        results, attn = self.net_G(z, f_m, f_e, mask)\n",
        "        self.img_rec = []\n",
        "        self.img_g = []\n",
        "        for result in results:\n",
        "            img_rec, img_g = result.chunk(2)\n",
        "            self.img_rec.append(img_rec)\n",
        "            self.img_g.append(img_g)\n",
        "        self.img_out = (1-self.mask) * self.img_g[-1].detach() + self.mask * self.img_truth\n",
        "\n",
        "    def backward_D_basic(self, netD, real, fake):\n",
        "        \"\"\"Calculate GAN loss for the discriminator\"\"\"\n",
        "        # Real\n",
        "        D_real = netD(real)\n",
        "        D_real = DiffAugment(D_real, policy=policy)\n",
        "\n",
        "        D_real_loss = self.GANloss(D_real, True, True)\n",
        "        # fake\n",
        "        D_fake = netD(fake.detach())\n",
        "        D_fake = DiffAugment(D_fake, policy=policy)\n",
        "\n",
        "        D_fake_loss = self.GANloss(D_fake, False, True)\n",
        "        # loss for discriminator\n",
        "        D_loss = (D_real_loss + D_fake_loss) * 0.5\n",
        "        # gradient penalty for wgan-gp\n",
        "        if self.opt.gan_mode == 'wgangp':\n",
        "            gradient_penalty, gradients = external_function.cal_gradient_penalty(netD, real, fake.detach())\n",
        "            D_loss +=gradient_penalty\n",
        "\n",
        "        D_loss.backward()\n",
        "\n",
        "        return D_loss\n",
        "\n",
        "    def backward_D(self):\n",
        "        \"\"\"Calculate the GAN loss for the discriminators\"\"\"\n",
        "        base_function._unfreeze(self.net_D, self.net_D_rec)\n",
        "        self.loss_img_d = self.backward_D_basic(self.net_D, self.img_truth, self.img_g[-1])\n",
        "        self.loss_img_d_rec = self.backward_D_basic(self.net_D_rec, self.img_truth, self.img_rec[-1])\n",
        "\n",
        "    def backward_G(self):\n",
        "        \"\"\"Calculate training loss for the generator\"\"\"\n",
        "\n",
        "        # encoder kl loss\n",
        "        self.loss_kl_rec = self.kl_rec.mean() * self.opt.lambda_kl * self.opt.output_scale\n",
        "        self.loss_kl_g = self.kl_g.mean() * self.opt.lambda_kl * self.opt.output_scale\n",
        "\n",
        "        # generator adversarial loss\n",
        "        base_function._freeze(self.net_D, self.net_D_rec)\n",
        "        # g loss fake\n",
        "        D_fake = self.net_D(self.img_g[-1])\n",
        "        D_fake = DiffAugment(D_fake, policy=policy)\n",
        "\n",
        "        self.loss_ad_g = self.GANloss(D_fake, True, False) * self.opt.lambda_g\n",
        "\n",
        "        # rec loss fake\n",
        "        D_fake = self.net_D_rec(self.img_rec[-1])\n",
        "        D_fake = DiffAugment(D_fake, policy=policy)\n",
        "\n",
        "        D_real = self.net_D_rec(self.img_truth)\n",
        "        D_real = DiffAugment(D_real, policy=policy)\n",
        "\n",
        "        self.loss_ad_rec = self.L2loss(D_fake, D_real) * self.opt.lambda_g\n",
        "\n",
        "        # calculate l1 loss ofr multi-scale outputs\n",
        "        loss_app_rec, loss_app_g = 0, 0\n",
        "        for i, (img_rec_i, img_fake_i, img_real_i, mask_i) in enumerate(zip(self.img_rec, self.img_g, self.scale_img, self.scale_mask)):\n",
        "            loss_app_rec += self.L1loss(img_rec_i, img_real_i)\n",
        "            if self.opt.train_paths == \"one\":\n",
        "                loss_app_g += self.L1loss(img_fake_i, img_real_i)\n",
        "            elif self.opt.train_paths == \"two\":\n",
        "                loss_app_g += self.L1loss(img_fake_i*mask_i, img_real_i*mask_i)\n",
        "        self.loss_app_rec = loss_app_rec * self.opt.lambda_rec\n",
        "        self.loss_app_g = loss_app_g * self.opt.lambda_rec\n",
        "\n",
        "        # if one path during the training, just calculate the loss for generation path\n",
        "        if self.opt.train_paths == \"one\":\n",
        "            self.loss_app_rec = self.loss_app_rec * 0\n",
        "            self.loss_ad_rec = self.loss_ad_rec * 0\n",
        "            self.loss_kl_rec = self.loss_kl_rec * 0\n",
        "\n",
        "        total_loss = 0\n",
        "\n",
        "        for name in self.loss_names:\n",
        "            if name != 'img_d' and name != 'img_d_rec':\n",
        "                total_loss += getattr(self, \"loss_\" + name)\n",
        "\n",
        "        total_loss.backward()\n",
        "\n",
        "    def optimize_parameters(self):\n",
        "        \"\"\"update network weights\"\"\"\n",
        "        # compute the image completion results\n",
        "        self.forward()\n",
        "        # optimize the discrinimator network parameters\n",
        "        self.optimizer_D.zero_grad()\n",
        "        self.backward_D()\n",
        "        self.optimizer_D.step()\n",
        "        # optimize the completion network parameters\n",
        "        self.optimizer_G.zero_grad()\n",
        "        self.backward_G()\n",
        "        self.optimizer_G.step()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq2kcXV2rGwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://github.com/lyndonzheng/Pluralistic-Inpainting/blob/23059b3488a6019d47425b2476632265699718c1/options/train_options.py\n",
        "!python train.py --name sudo --img_file /content/input/ --checkpoints_dir /content/model-checkpoints --save_iters_freq 1000"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}